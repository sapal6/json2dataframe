{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from fastai.text import *\nimport numpy as np\nimport json\n\n%matplotlib inline","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Let's Set All The Paths? \nLet's set all the path which is required to work on our data.\n\n* `path` is the root input directory.\n* `outputPath` is where we would be saving our csv , models etc.\n* `datasetPath` is the directory to the dataset."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"inputPath = Path('/kaggle/input')\noutputPath = Path('/kaggle/working')\n#datasetPath = path/'covid19-tweets/'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"inputPath.ls()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!pip install twarc","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"consumer_key= \"3AO7qGIh76LUgZgcXs0VNWtjC\"\nconsumer_secret= \"DJFneehCiWKbLbqqFfBeYZuu1WbRoFqd2LEL9VTeMHGwRCkc5U\"\naccess_token= \"804532808190291969-ON0c25lsbJg26regleTnEb1SypMhLk7\"\naccess_token_secret= \"yN6R535MAD3kf4tlhU2s6DGP2CCVosbJZfm7mxgCSjjtY\"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import gzip\nimport json\n\nfrom tqdm import tqdm\nfrom twarc import Twarc\nfrom pathlib import Path\n\ntwarc = Twarc(consumer_key, consumer_secret, access_token, access_token_secret)\n#data_dirs = ['2020-01', '2020-02','2020-03', '2020-04']\ndata_dirs = ['2020-01']\n\n\ndef main():\n    for data_dir in data_dirs:\n        for path in Path(inputPath/data_dir).iterdir():\n            if path.name.endswith('.txt'):\n                hydrate(path, data_dir)\n\n\ndef _reader_generator(reader):\n    b = reader(1024 * 1024)\n    while b:\n        yield b\n        b = reader(1024 * 1024)\n\n\ndef raw_newline_count(fname):\n    \"\"\"\n    Counts number of lines in file\n    \"\"\"\n    f = open(fname, 'rb')\n    f_gen = _reader_generator(f.raw.read)\n    return sum(buf.count(b'\\n') for buf in f_gen)\n\n\ndef hydrate(id_file, data_dir):\n    print('hydrating {}'.format(id_file))\n\n    #gzip_path = id_file.with_suffix('.jsonl.gz')\n    gzip_path = (outputPath/id_file.name).with_suffix('.jsonl.gz')\n    if gzip_path.is_file():\n        print('skipping json file already exists: {}'.format(gzip_path))\n        return\n\n    num_ids = raw_newline_count(id_file)\n\n    with gzip.open(gzip_path, 'w') as output:\n        with tqdm(total=num_ids) as pbar:\n            for tweet in twarc.hydrate(id_file.open()):\n                output.write(json.dumps(tweet).encode('utf8') + b\"\\n\")\n                pbar.update(1)\n\n\nif __name__ == \"__main__\":\n    main()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"some of the code ceels are converted to markdown to prevent htme from getting executed in the committed kernel. These cells should be converted back to code when this kernel would be forked for further developement."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# load_files(fileNames: tuple):\n# fileList = []\n\n# for file in fileNames:\n#     fileList.append(file)\n#     \n# return fileList"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"# uncomment this line if you want both folders.\n#files = load_files((pmcCustomLicense,biorxivMedrxiv))\n#files = load_files((biorxivMedrxiv.iterdir()))"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"type(files)"},{"metadata":{},"cell_type":"markdown","source":"To check the sample of the contents of the processed list run the following code."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"files[:3]"},{"metadata":{},"cell_type":"markdown","source":"The file paths processed with the `load_files()` contains path objects and may not be suitable for certain operations. For example path objects can't be iterated over. \n\nWe will convert those path objects into their string \"paths\" with the `filePath()` function. This functions does the following -->\n* Takes in the list havin gthe path objects.\n* Iterates through the list and converts each list item i.e. the path objects into string format.\n* Then it puts those string paths into the `filePath` list."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"def filePath(files):\n    filePath = []\n    \n    for file in files:\n        filePath.append(str(file))\n        \n    return filePath"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"filePaths=filePath(files)"},{"metadata":{},"cell_type":"markdown","source":"The `getRawFiles()` functions does the following -->\n* Takes in a list of file paths\n* Goes through each file path from the list `files` and uses the `json.load()` method to read the contents.\n* finally it appends these json object into the `rawFiles` list."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"def getRawFiles(files: list):\n    rawFiles = []\n        \n    for fileName in files:\n            rawFile = json.load(open(fileName, 'rb'))\n            rawFiles.append(rawFile)\n            \n    return rawFiles"},{"metadata":{"trusted":true},"cell_type":"code","source":"rawFiles = getRawFiles(filePaths)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"`format_name()`-->\n* Takes in the json object with author as the key.\n* Joins the names in the following sequence <first name> <middle name> <last name.\n* Function also takes care o fthe fact that if there no middle name then just do <first name> <last name\n\n`format_affiliation` -->\n* Takes in the json object with key as affiliation.\n* If location details are there in json then put it into a list and return it.\n* If institution is there then join location and instituion details together and put it into a list.\n\n`format_authors()` -->\n* Takes in the json object with key as author.\n* Joins the author's name with the affiliations if affiliations are available.\n\n`format_body()` -->\n* Takes in the json object with key as body_text.\n* Extracts the text and then appends it into a list.\n\n`format_bib()` -->\n* Takes in the json object with key as bib.\n* Joins the 'title', 'authors', 'venue', 'year' together to form a string."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"def format_name(author):\n    middle_name = \" \".join(author['middle'])\n    \n    if author['middle']:\n        return \" \".join([author['first'], middle_name, author['last']])\n    else:\n        return \" \".join([author['first'], author['last']])\n\n\ndef format_affiliation(affiliation):\n    text = []\n    location = affiliation.get('location')\n    if location:\n        text.extend(list(affiliation['location'].values()))\n    \n    institution = affiliation.get('institution')\n    if institution:\n        text = [institution] + text\n    return \", \".join(text)\n\ndef format_authors(authors, with_affiliation=False):\n    name_ls = []\n    \n    for author in authors:\n        name = format_name(author)\n        if with_affiliation:\n            affiliation = format_affiliation(author['affiliation'])\n            if affiliation:\n                name_ls.append(f\"{name} ({affiliation})\")\n            else:\n                name_ls.append(name)\n        else:\n            name_ls.append(name)\n    \n    return \", \".join(name_ls)\n\ndef format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n    \n    for section, text in texts:\n        texts_di[section] += text\n\n    body = \"\"\n\n    for section, text in texts_di.items():\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"\n    \n    return body\n\ndef format_bib(bibs):\n    if type(bibs) == dict:\n        bibs = list(bibs.values())\n    bibs = deepcopy(bibs)\n    formatted = []\n    \n    for bib in bibs:\n        bib['authors'] = format_authors(\n            bib['authors'], \n            with_affiliation=False\n        )\n        formatted_ls = [str(bib[k]) for k in ['title', 'authors', 'venue', 'year']]\n        formatted.append(\", \".join(formatted_ls))\n\n    return \"; \".join(formatted)"},{"metadata":{},"cell_type":"markdown","source":"`generate_clean_df()` -->\n* Uses the above helper functions to create a pandas dataframe.\n* The resulting dataframe would have the following columns ->\n  *'paper_id'\n  * 'title'\n  * 'authors'\n  * 'affiliations'\n  * 'abstract'\n  * 'text'\n  * 'bibliography'\n  * 'raw_authors'\n  * 'raw_bibliography'"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"def generate_clean_df(all_files):\n    cleaned_files = []\n    \n    for file in all_files:\n        features = [\n            file['paper_id'],\n            file['metadata']['title'],\n            format_authors(file['metadata']['authors']),\n            format_authors(file['metadata']['authors'], \n                           with_affiliation=True),\n            format_body(file['abstract']),\n            format_body(file['body_text']),\n            format_bib(file['bib_entries']),\n            file['metadata']['authors'],\n            file['bib_entries']\n        ]\n\n        cleaned_files.append(features)\n\n    col_names = ['paper_id', 'title', 'authors',\n                 'affiliations', 'abstract', 'text', \n                 'bibliography','raw_authors','raw_bibliography']\n\n    clean_df = pd.DataFrame(cleaned_files, columns=col_names)\n    clean_df.head()\n    \n    return clean_df"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"pd.set_option('display.max_columns', None) \ncleanDf = generate_clean_df(rawFiles)\ncleanDf.head()"},{"metadata":{},"cell_type":"markdown","source":"Finally save the dataframe to a csv file. Since the entire previous process takes a bit of time, so saving the file to csv saves time later."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"cleanDf.to_csv(outputPath/'cleandf.csv')"},{"metadata":{},"cell_type":"markdown","source":"## The Data \nWe then create a databunch from the csv that we created from the json files.\n\nThe `createDataBunchForLanguageModel()` is a helper function. This does the following -->\n* Creates a databunch from the csv file.\n* The data in the databunch is fetched from the 'text' column of the csv file.\n* 10% of the data is reserved as the validation data.\n* Since the language model that we will create down the line will be a self-supervised model. It takes in the label from teh data itself.\n* Thus the `label_for_lm()` helps us with that.\n* After the databunch is created, it is saved to a pickle file. Next time when this notebook is run then we don't need to go through the process of creation of the databunch once again. We can just use the pickle file."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"def createDataBunchForLanguageModel(outputPath: Path,\n                                    csvName: str,\n                                    textCol: str, \n                                    pickelFileName: str,\n                                    splitBy: float, \n                                   batchSize: int):\n    data_lm = TextList.from_csv(outputPath,\n                                f'{csvName}.csv',\n                                cols=textCol)\\\n                  .split_by_rand_pct(splitBy)\\\n                  .label_for_lm()\\\n                  .databunch(bs=batchSize)\n    \n    data_lm.save(f'{pickelFileName}.pkl')"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"createDataBunchForLanguageModel(outputPath,\n                                    'cleandf',\n                                    'text', \n                                    'cleanDf',\n                                    0.1,\n                                    48)"},{"metadata":{},"cell_type":"markdown","source":"The `loadData()` is a helper function to load the databunch file. It takes in the required batch size which we want to load from the databunch."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"def loadData(outputPath: Path,\n             databunchFileName: str,\n             batchSize: int,\n             showBatch: bool= False):\n    \n    data_lm = load_data(outputPath,\n                       f'{databunchFileName}.pkl',\n                       bs=batchSize)\n    \n    if showBatch:\n        data_lm.show_batch()\n        \n    return data_lm"},{"metadata":{},"cell_type":"markdown","source":"## Building The Learner \nWe use ULMFIT to create a language model on the research data corpus and then fine tune this language model.\n\nWe create the language model learner. The `language_model_learner()` is a fastai method which helps us to build a learner object with the data created in the previous sections. \n\nHere we use a batch size of 48. This learner is created with a pretrained language model architecure `AWD_LSTM`. This is the self supervised part. This is a model which was trained on a english language corpus to predict the next sequence of sentences and thus understands the structure of the language.\n\nWe just need to fine tune this modle on our data corpus which then can be used to build a classifier.\n\nI am not buidling any classifier as yet because I don't have any idea as to what we need to classify. However I believe that this fine tuned language modle can be customized to find hidden information in the large corpus of research papers which would otherwise be hidden/missed by the human readers."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"learner = language_model_learner(loadData(outputPath,\n             'cleanDf',\n             48,\n             showBatch= False),\n             AWD_LSTM,\n             drop_mult=0.3)"},{"metadata":{},"cell_type":"markdown","source":"We plot the learning rate for our language model. From this plot we will try to find the best learnign rate suitable for our model. \nThe `plotLearningRate()` is a helper function which plots the learnign rate for us."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"def plotLearningRate(learner, skip_end):\n    learner.lr_find()\n    learner.recorder.plot(skip_end=skip_end)"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"plotLearningRate(learner, 15)"},{"metadata":{},"cell_type":"markdown","source":"We then take in that learning rate as our start from where the plot diverges. The model is then trained with the \"fit one cycle\" with this strating learning rate.\n\nThis is where we train the head only."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"learner.fit_one_cycle(1, 1e-02, moms=(0.8,0.7))"},{"metadata":{},"cell_type":"markdown","source":"Since we are creating a language model, we are not overly concerned about getting the best possible accuracy here. So, we unfreeze the network and train some more."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"learner.unfreeze()\nlearner.fit_one_cycle(10,1e-3, moms=(0.8,0.7))"},{"metadata":{},"cell_type":"markdown","source":"Finally we save the fine tuned language model for use in testing/prediction."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"learner.save('fineTuned')"},{"metadata":{},"cell_type":"markdown","source":"## Testing \nLet's see if the model can connect the information/knowledge from the corpus. We first load the saved model and then try to find prediction for a search term."},{"metadata":{"trusted":true},"cell_type":"code","source":"learner = learner.load('fineTuned')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"TEXT = \"Range of incubation periods for the disease in humans\"\nN_WORDS = 40\nN_SENTENCES = 2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"\\n\".join(learner.predict(TEXT, N_WORDS, temperature=0.75) for _ in range(N_SENTENCES)))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We have a pretty good language model available now. Now, that we have our language modle fine tuned, we can save the modle which predicts the next sequence of words and the encoder which is responsible for creating and updating the hidden states.\n\nWe will use `.save()` to save the `.pth` file."},{"metadata":{"trusted":true},"cell_type":"markdown","source":"def save(learner,\n         saveEncoder: bool = True):\n    \n    if saveEncoder:\n        learner.save_encoder('fine_tuned_encoder')\n        \n    learner.save('fine_tuned_model')"},{"metadata":{"trusted":true},"cell_type":"markdown","source":"save(learner)"},{"metadata":{},"cell_type":"markdown","source":"## End Notes \nI may not have completed any dataset tasks with this kernel but I hope that this kernel and the language model built in this kernel would be helpful for someone else who might be working on the dataset tasks. This might ultimately help us to gain some insights into the research and this could ultimately help us to fight this dreadful disease.\n\nI am in no way an expert in NLP so the network that I have developed here is based on the code from the lesson3 of the course - [\"Practicel Deep Learning For coders\" by fastai](https://www.fast.ai/2019/01/24/course-v3/). \n\nI am hoping that someone who is more seasoned in NLP and deep learning would improve on this model and bring out something useful which could then contibute towards fighting COVID-19."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}